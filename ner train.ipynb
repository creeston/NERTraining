{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    \n",
    "    tweet_tokens = []\n",
    "    tweet_tags = []\n",
    "    for line in open(file_path, encoding='utf-8'):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            if tweet_tokens:\n",
    "                tokens.append(tweet_tokens)\n",
    "                tags.append(tweet_tags)\n",
    "            tweet_tokens = []\n",
    "            tweet_tags = []\n",
    "        else:\n",
    "            token, tag = line.split()\n",
    "            # Replace all urls with <URL> token\n",
    "            # Replace all users with <USR> token\n",
    "\n",
    "            ######################################\n",
    "            ######### YOUR CODE HERE #############\n",
    "            ######################################\n",
    "            if token.startswith('@'):\n",
    "                token = '<USR>'\n",
    "            if token.lower().startswith('http://') or token.lower().startswith('https://'):\n",
    "                token = '<URL>'\n",
    "            \n",
    "            tweet_tokens.append(token)\n",
    "            tweet_tags.append(tag)\n",
    "            \n",
    "    return tokens, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens, train_tags = read_data('C:\\\\coursera-hse-nlp-master\\\\coursera-hse-nlp-master\\\\week2\\data\\\\train.txt')\n",
    "validation_tokens, validation_tags = read_data('C:\\\\coursera-hse-nlp-master\\\\coursera-hse-nlp-master\\\\week2\\data\\\\validation.txt')\n",
    "test_tokens, test_tags = read_data('C:\\\\coursera-hse-nlp-master\\\\coursera-hse-nlp-master\\\\week2\\\\data\\\\test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I-product'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens[1][1]\n",
    "train_tags[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_dict(tokens_or_tags, special_tokens):\n",
    "    tok2idx = defaultdict(lambda: 0)\n",
    "    idx2tok = []\n",
    "    tokens = list(set([token for tweet in tokens_or_tags for token in tweet]))\n",
    "    vocab = special_tokens + tokens\n",
    "    \n",
    "    for i,token in enumerate(vocab):\n",
    "        tok2idx[token] = i\n",
    "        idx2tok.append(token)\n",
    "    \n",
    "    return tok2idx, idx2tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = ['<UNK>', '<PAD>']\n",
    "special_tags = ['O']\n",
    "\n",
    "token2idx, idx2token = build_dict(train_tokens + validation_tokens, special_tokens)\n",
    "tag2idx, idx2tag = build_dict(train_tags, special_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20505"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words2idxs(tokens_list):\n",
    "    return [token2idx[word] for word in tokens_list]\n",
    "\n",
    "def tags2idxs(tags_list):\n",
    "    return [tag2idx[tag] for tag in tags_list]\n",
    "\n",
    "def idxs2words(idxs):\n",
    "    return [idx2token[idx] for idx in idxs]\n",
    "\n",
    "def idxs2tags(idxs):\n",
    "    return [idx2tag[idx] for idx in idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def batches_generator(batch_size, tokens, tags, shuffle=True, allow_smaller_last_batch=True):\n",
    "    n_samples = len(tokens)\n",
    "    if shuffle:\n",
    "        order = np.random.permutation(n_samples)\n",
    "    else:\n",
    "        order = np.arange(n_samples)\n",
    "\n",
    "    n_batches = n_samples // batch_size\n",
    "    if allow_smaller_last_batch and n_samples % batch_size:\n",
    "        n_batches += 1\n",
    "\n",
    "    for k in range(n_batches):\n",
    "        batch_start = k * batch_size\n",
    "        batch_end = min((k + 1) * batch_size, n_samples)\n",
    "        current_batch_size = batch_end - batch_start\n",
    "        x_list = []\n",
    "        y_list = []\n",
    "        max_len_token = 0\n",
    "        for idx in order[batch_start: batch_end]:\n",
    "            x_list.append(words2idxs(tokens[idx]))\n",
    "            y_list.append(tags2idxs(tags[idx]))\n",
    "            max_len_token = max(max_len_token, len(tags[idx]))\n",
    "            \n",
    "        # Fill in the data into numpy nd-arrays filled with padding indices.\n",
    "        x = np.ones([current_batch_size, max_len_token], dtype=np.int32) * token2idx['<PAD>']\n",
    "        y = np.ones([current_batch_size, max_len_token], dtype=np.int32) * tag2idx['O']\n",
    "        lengths = np.zeros(current_batch_size, dtype=np.int32)\n",
    "        for n in range(current_batch_size):\n",
    "            utt_len = len(x_list[n])\n",
    "            x[n, :utt_len] = x_list[n]\n",
    "            lengths[n] = utt_len\n",
    "            y[n, :utt_len] = y_list[n]\n",
    "        yield x, y, lengths\n",
    "        \n",
    "def pad_text(X, Y):\n",
    "    new_X = []\n",
    "    new_Y = []\n",
    "    for tags in Y:\n",
    "        new_Y.append(tags2idxs(tags))\n",
    "    new_Y = pad_sequences(new_Y, padding='post', value=tag2idx['O'])\n",
    "    max_len = len(new_Y[0])\n",
    "    for seq in X:\n",
    "        new_seq = []\n",
    "        new_seq.extend(seq)\n",
    "        rest = max_len - len(new_seq)\n",
    "        if rest != 0:\n",
    "            new_seq.extend(['<PAD>'] * rest)\n",
    "        new_X.append(new_seq)\n",
    "    return np.array(new_X), new_Y    \n",
    "        \n",
    "def pad_data(X, Y, max_len):\n",
    "    new_X = []\n",
    "    new_Y = []\n",
    "    for seq, tags in zip(X, Y):\n",
    "        new_X.append(words2idxs(seq))\n",
    "        new_Y.append(tags2idxs(tags))\n",
    "    new_X = pad_sequences(new_X, padding='post', value=token2idx['<PAD>'])\n",
    "    new_Y = pad_sequences(new_Y, padding='post', value=tag2idx['O'])\n",
    "    return new_X, new_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist([len(t) for t in train_tokens + validation_tokens], bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "from keras import backend as K\n",
    "\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.tables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ElmoEmbedding(x):\n",
    "    return elmo_model(\n",
    "        inputs={\"tokens\": tf.squeeze(tf.cast(x, tf.string)),\"sequence_len\": tf.constant(batch_size*[max_len])},\n",
    "        signature=\"tokens\",\n",
    "        as_dict=True)[\"elmo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_17 (Embedding)     (None, None, 200)         4101000   \n",
      "_________________________________________________________________\n",
      "bidirectional_51 (Bidirectio (None, None, 100)         100400    \n",
      "_________________________________________________________________\n",
      "bidirectional_52 (Bidirectio (None, 100)               60400     \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 41)                4141      \n",
      "=================================================================\n",
      "Total params: 4,265,941\n",
      "Trainable params: 4,265,941\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Lambda, Add, InputLayer\n",
    "\n",
    "max_len = 41\n",
    "n_tags = len(idx2tag)\n",
    "n_words = len(idx2token)\n",
    "embedding_dim = 200\n",
    "batch_size = 32\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    #InputLayer(input_shape=(41,), batch_size=batch_size, dtype=tf.string),\n",
    "    Embedding(input_dim=n_words, output_dim=embedding_dim, mask_zero=True),\n",
    "    #Lambda(ElmoEmbedding, output_shape=(max_len, 1024)),\n",
    "    Bidirectional(LSTM(50, return_sequences=True), input_shape=(embedding_dim, max_len)),\n",
    "    Bidirectional(LSTM(50)),\n",
    "    Dense(max_len, activation=\"relu\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pad_data() missing 1 required positional argument: 'max_len'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-228-f72539d155fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_tr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_tokens\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mvalidation_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_tags\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mvalidation_tags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: pad_data() missing 1 required positional argument: 'max_len'"
     ]
    }
   ],
   "source": [
    "X_tr, y_tr = pad_data(train_tokens + validation_tokens, train_tags + validation_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6519 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-229-ae11b2cb6aff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 675\u001b[1;33m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3459\u001b[0m         \u001b[0mtensor_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3460\u001b[0m         array_vals.append(np.asarray(value,\n\u001b[1;32m-> 3461\u001b[1;33m                                      dtype=tensor_type.as_numpy_dtype))\n\u001b[0m\u001b[0;32m   3462\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3463\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'data'"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_tr, y_tr, batch_size=batch_size, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6071375449695412080\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
